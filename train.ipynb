{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import dill\n",
    "import os\n",
    "import settings as s\n",
    "import time\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import torchtext\n",
    "\n",
    "import torch.optim as O\n",
    "import torch.nn as nn\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from model import BinarySARNN\n",
    "from configs import *\n",
    "from utils import dir_to_csv, tokenize, binary_accuracy, epoch_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files_path = os.path.join(s.DATASET_PATH, s.TRAIN_DIR)\n",
    "test_files_path = os.path.join(s.DATASET_PATH, s.TEST_DIR)\n",
    "dataset_path = os.path.join(s.DATA_DIR, s.CSV)\n",
    "\n",
    "train_file = os.path.join(s.DATA_DIR, \"train.csv\")\n",
    "val_file = os.path.join(s.DATA_DIR, \"val.csv\")\n",
    "test_file = os.path.join(s.DATA_DIR, \"test.csv\")\n",
    "\n",
    "d_embedding = s.D_EMBEDDING\n",
    "\n",
    "embeddings_path = os.path.join(s.DATASET_PATH, s.EMBEDDINGS_FILE)\n",
    "vocab_path = os.path.join(s.DATA_DIR, s.VOCAB_FILE)\n",
    "\n",
    "model_config = LSTM_CONFIG3\n",
    "train_config = TRAIN_CONFIG5\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ds_to_csv(ds, file):\n",
    "    if not os.path.isfile(file):\n",
    "        with open(file, \"w+\") as csvf:\n",
    "            writer = csv.writer(csvf,\n",
    "                                delimiter=\" \",\n",
    "                                quoting=csv.QUOTE_NONNUMERIC)\n",
    "            for e in ds:  \n",
    "                writer.writerow([\" \".join(e.review), e.label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists, skipping creation...\n",
      "Loading dataset...\n",
      "Split already done, loading...\n",
      "Train and Validation sets successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "# create train set\n",
    "dataset_paths = [train_files_path, test_files_path]\n",
    "dir_to_csv(s.CSV, dataset_paths)\n",
    "  \n",
    "LABEL = torchtext.data.Field(sequential=False, use_vocab=False)\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "if not (os.path.isfile(train_file) or os.path.isfile(val_file) or os.path.isfile(test_file)):\n",
    "    \n",
    "    REVIEW = torchtext.data.Field(tokenize=tokenize, lower=True, include_lengths=True)\n",
    "    \n",
    "    print(\"Splitting train, val and test sets...\")\n",
    "    dataset = torchtext.data.TabularDataset(path=dataset_path, format=\"CSV\",\n",
    "                                          fields=[(\"review\", REVIEW),\n",
    "                                                  (\"label\", LABEL)],\n",
    "                                          csv_reader_params={\"delimiter\": \" \"})\n",
    "\n",
    "    train, val, test = dataset.split(split_ratio=[0.7, 0.2, 0.1])\n",
    "    \n",
    "    #serialize train, val and test\n",
    "    ds_to_csv(train, train_file)\n",
    "    ds_to_csv(val, val_file)\n",
    "    ds_to_csv(test, test_file)\n",
    "    \n",
    "    REVIEW.build_vocab(train)\n",
    "    \n",
    "    with open(os.path.join(s.DATA_DIR, \"REVIEW.field\"), \"wb+\") as reviewf:\n",
    "        dill.dump(REVIEW, reviewf)\n",
    "        \n",
    "    print(\"Train and Validation sets successfully created.\")\n",
    "    \n",
    "else:\n",
    "    print(\"Split already done, loading...\")\n",
    "    \n",
    "    with open(os.path.join(s.DATA_DIR, \"REVIEW.field\"), \"rb\") as reviewf:\n",
    "        REVIEW = dill.load(reviewf)\n",
    "        \n",
    "    train = torchtext.data.TabularDataset(path=train_file, format=\"CSV\",\n",
    "                                          fields=[(\"review\", REVIEW),\n",
    "                                                  (\"label\", LABEL)],\n",
    "                                          csv_reader_params={\"delimiter\": \" \"})\n",
    "    \n",
    "    val = torchtext.data.TabularDataset(path=val_file, format=\"CSV\",\n",
    "                                          fields=[(\"review\", REVIEW),\n",
    "                                                  (\"label\", LABEL)],\n",
    "                                          csv_reader_params={\"delimiter\": \" \"})\n",
    "    \n",
    "    test = torchtext.data.TabularDataset(path=test_file, format=\"CSV\",\n",
    "                                          fields=[(\"review\", REVIEW),\n",
    "                                                  (\"label\", LABEL)],\n",
    "                                          csv_reader_params={\"delimiter\": \" \"})\n",
    "    print(\"Train and Validation sets successfully loaded.\")\n",
    "\n",
    "\n",
    "train_iter = torchtext.data.Iterator(train, model_config.batch_size, device=device, sort_within_batch = True,\n",
    "                                     sort_key=lambda x: len(x.review))\n",
    "val_iter = torchtext.data.Iterator(val, model_config.batch_size, device=device, sort_within_batch = True,\n",
    "                                   sort_key=lambda x: len(x.review))\n",
    "\n",
    "test_iter = torchtext.data.Iterator(test, 64, device=device, sort_within_batch = True,\n",
    "                               sort_key=lambda x: len(x.review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings successfully loaded.\n",
      "\n",
      "Vocabulary file already exists, skipping creation.\n"
     ]
    }
   ],
   "source": [
    "# load word embeddings\n",
    "print(\"Loading word embeddings...\")\n",
    "embeddings = KeyedVectors.load_word2vec_format(embeddings_path, binary=False,\n",
    "                                               unicode_errors='ignore')\n",
    "print(\"Word embeddings successfully loaded.\\n\")\n",
    "\n",
    "vocab = embeddings.vocab\n",
    "if not os.path.isfile(vocab_path):\n",
    "    print(\"Vocabulary file not present, creating...\")\n",
    "    with open(vocab_path, \"wb+\") as vf:\n",
    "        pickle.dump(vocab, vf)\n",
    "    print(\"Done.\\n\")\n",
    "else:\n",
    "    print(\"Vocabulary file already exists, skipping creation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BinarySARNN(\n",
       "  (embed): Embedding(168994, 300)\n",
       "  (rnn): LSTM(300, 128, num_layers=2, bidirectional=True)\n",
       "  (hidden_to_label): Linear(in_features=256, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set model configurations\n",
    "model_config.vocab_size = len(vocab)\n",
    "model_config.d_embed = d_embedding\n",
    "\n",
    "model = BinarySARNN(model_config)\n",
    "\n",
    "model.embed.weight.data.copy_(torch.from_numpy(embeddings.vectors))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_IDX = REVIEW.vocab.stoi[REVIEW.unk_token]\n",
    "PAD_IDX = REVIEW.vocab.stoi[REVIEW.pad_token]\n",
    "\n",
    "model.embed.weight.data[UNK_IDX] = torch.zeros(model_config.d_embed)\n",
    "model.embed.weight.data[PAD_IDX] = torch.zeros(model_config.d_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Config\n",
      "============\n",
      "d_hidden = 128\n",
      "vocab_size = 168994\n",
      "d_embed = 300\n",
      "batch_size = 64\n",
      "n_layers = 2\n",
      "nonlin = \n",
      "dropout = 0\n",
      "bidir = True\n",
      "arch = LSTM\n",
      "\n",
      "Train Config\n",
      "=============\n",
      "criterion = <class 'torch.nn.modules.loss.CrossEntropyLoss'>\n",
      "optimizer = <class 'torch.optim.rmsprop.RMSprop'>\n",
      "optimizer args = {}\n",
      "epochs = 5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# set training configuration\n",
    "criterion = train_config.criterion()\n",
    "criterion.to(device)\n",
    "\n",
    "opt = train_config.optimizer(model.parameters(), **train_config.o_kwargs)\n",
    "\n",
    "print(str(model_config) + \"\\n\")\n",
    "print(str(train_config) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to /home/jupyter/models/aclImdb/lstm_2layers_bidir|rmsprop.pt\n",
      "Epoch: 01 | Epoch Time: 4m 54s\n",
      "\tTrain Loss: 0.518 | Train Acc: 73.21%\n",
      "\t Val. Loss: 0.529 |  Val. Acc: 72.77%\n",
      "Saving to /home/jupyter/models/aclImdb/lstm_2layers_bidir|rmsprop.pt\n",
      "Epoch: 02 | Epoch Time: 9m 50s\n",
      "\tTrain Loss: 0.348 | Train Acc: 85.71%\n",
      "\t Val. Loss: 0.326 |  Val. Acc: 86.23%\n",
      "Epoch: 03 | Epoch Time: 14m 42s\n",
      "\tTrain Loss: 0.223 | Train Acc: 91.07%\n",
      "\t Val. Loss: 0.345 |  Val. Acc: 85.70%\n",
      "Epoch: 04 | Epoch Time: 19m 36s\n",
      "\tTrain Loss: 0.060 | Train Acc: 96.43%\n",
      "\t Val. Loss: 0.392 |  Val. Acc: 85.82%\n",
      "Epoch: 05 | Epoch Time: 24m 28s\n",
      "\tTrain Loss: 0.066 | Train Acc: 96.43%\n",
      "\t Val. Loss: 0.598 |  Val. Acc: 83.03%\n",
      "Best Epoch -> 02\n"
     ]
    }
   ],
   "source": [
    "iterations = 0\n",
    "start = time.time()\n",
    "best_val_acc = -1\n",
    "val_every = 100\n",
    "train_iter.repeat = False\n",
    "best_epoch = 1\n",
    "\n",
    "model_fname = f\"{model_config.id}|{train_config.id}.pt\"\n",
    "\n",
    "for epoch in range(train_config.epochs):\n",
    "    train_iter.init_epoch()\n",
    "    #print(torch.cuda.memory_allocated(device=device))\n",
    "    #print(\"train...\")\n",
    "    model.train()\n",
    "    for batch_idx, batch in enumerate(train_iter):\n",
    "        #print(torch.cuda.memory_allocated(device=device))\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        \n",
    "        iterations += 1\n",
    "        #forward pass\n",
    "        #print(batch.review)\n",
    "        reviews, review_lengths = batch.review\n",
    "        answer = model(reviews, review_lengths)\n",
    "        \n",
    "        #calculate accuracy in current batch\n",
    "        #print(torch.max(answer, 1)[1], \" vs\", batch.label)\n",
    "        train_acc = binary_accuracy(torch.max(answer, 1)[1], batch.label).item()\n",
    "        #calculate loss\n",
    "        loss = criterion(answer, batch.label)\n",
    "\n",
    "        #backpropagate, calculating gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        #update model parameters\n",
    "        opt.step()\n",
    "    #print(\"eval..\")\n",
    "    #evaluate\n",
    "    model.eval()\n",
    "    val_iter.init_epoch()\n",
    "    \n",
    "    epoch_loss, epoch_acc = 0, 0\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for val_batch_idx, val_batch in enumerate(val_iter):\n",
    "            #print(torch.cuda.memory_allocated(device=device))\n",
    "            reviews, review_lengths = val_batch.review\n",
    "            answer = model(reviews, review_lengths)\n",
    "            epoch_acc += binary_accuracy(torch.max(answer, 1)[1], val_batch.label).item()\n",
    "            epoch_loss += criterion(answer, val_batch.label)\n",
    "\n",
    "    epoch_loss /= len(val_iter)\n",
    "    epoch_acc /= len(val_iter)\n",
    "    torch.save(model.state_dict(), os.path.join(s.MODELS_PATH, f\"{model_fname}\"))\n",
    "    if epoch_acc > best_val_acc:\n",
    "        print(\"Saving to\", str(os.path.join(s.MODELS_PATH, model_fname)))\n",
    "        best_val_acc = epoch_acc\n",
    "        torch.save(model.state_dict(), os.path.join(s.MODELS_PATH, model_fname))\n",
    "        best_epoch = epoch\n",
    "                \n",
    "    epoch_mins, epoch_secs = epoch_time(start, time.time())\n",
    "    print(f\"Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\")\n",
    "    print(f\"\\tTrain Loss: {loss:.3f} | Train Acc: {train_acc*100:.2f}%\")\n",
    "    print(f'\\t Val. Loss: {epoch_loss:.3f} |  Val. Acc: {epoch_acc*100:.2f}%')\n",
    "print(f'Best Epoch -> {best_epoch+1:02}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archs =  {\"simple_rnn\": RNN_CONFIG1, \"simple_rnn_2layers\": RNN_CONFIG2, \"simple_rnn_2layers_bidir\": RNN_CONFIG3, \n",
    "         \"lstm\": LSTM_CONFIG1, \"lstm_2layers\": LSTM_CONFIG2, \"lstm_2layers_bidir\": LSTM_CONFIG3}\n",
    "optimizers = [\"sgd\", \"rmsprop\", \"adadelta\", \"adagrad\", \"adam\"]\n",
    "\n",
    "models = [\"|\".join([arch, optimizer]) for arch in archs for optimizer in optimizers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for arch in archs:\n",
    "    archs[arch].vocab_size = len(embeddings.vocab)\n",
    "    archs[arch].d_embed = embeddings.vector_size\n",
    "    \n",
    "criterion = nn.modules.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JUST IN CASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelb = BinarySARNN(LSTM_CONFIG3)\n",
    "modelb.load_state_dict(torch.load(os.path.join(s.MODELS_PATH, \"e3-lstm_2layers_bidir|adam.pt\")))\n",
    "modelb.to(device)\n",
    "\n",
    "modelc = BinarySARNN(LSTM_CONFIG3)\n",
    "modelc.load_state_dict(torch.load(os.path.join(s.MODELS_PATH, \"lstm_2layers_bidir|adam.pt\")))\n",
    "modelc.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.modules.CrossEntropyLoss()\n",
    "#test_iter = torchtext.data.Iterator(test, 64, device=device, sort_within_batch = True,\n",
    " #                              sort_key=lambda x: len(x.review))\n",
    "modelt = model\n",
    "modelt.eval()\n",
    "tloss, tacc = 0, 0\n",
    "with torch.no_grad():\n",
    "    for test_batch in test_iter:\n",
    "        treviews, treview_lengths = test_batch.review\n",
    "        tanswer = modelt(treviews, treview_lengths)\n",
    "        tacc += binary_accuracy(torch.max(tanswer, 1)[1], test_batch.label).item()\n",
    "        tloss += criterion(tanswer, test_batch.label)\n",
    "tloss /= len(test_iter)\n",
    "tacc /= len(test_iter)\n",
    "\n",
    "print(f\"\\tTest Loss: {tloss:.3f} | Test Acc: {tacc*100:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nbpresent": {
   "slides": {},
   "themes": {
    "default": "71a1c92b-9275-4d96-a282-d8f406725d8b",
    "theme": {
     "71a1c92b-9275-4d96-a282-d8f406725d8b": {
      "backgrounds": {
       "dc7afa04-bf90-40b1-82a5-726e3cff5267": {
        "background-color": "31af15d2-7e15-44c5-ab5e-e04b16a89eff",
        "id": "dc7afa04-bf90-40b1-82a5-726e3cff5267"
       }
      },
      "id": "71a1c92b-9275-4d96-a282-d8f406725d8b",
      "palette": {
       "19cc588f-0593-49c9-9f4b-e4d7cc113b1c": {
        "id": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "rgb": [
         252,
         252,
         252
        ]
       },
       "31af15d2-7e15-44c5-ab5e-e04b16a89eff": {
        "id": "31af15d2-7e15-44c5-ab5e-e04b16a89eff",
        "rgb": [
         68,
         68,
         68
        ]
       },
       "50f92c45-a630-455b-aec3-788680ec7410": {
        "id": "50f92c45-a630-455b-aec3-788680ec7410",
        "rgb": [
         197,
         226,
         245
        ]
       },
       "c5cc3653-2ee1-402a-aba2-7caae1da4f6c": {
        "id": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "rgb": [
         43,
         126,
         184
        ]
       },
       "efa7f048-9acb-414c-8b04-a26811511a21": {
        "id": "efa7f048-9acb-414c-8b04-a26811511a21",
        "rgb": [
         25.118061674008803,
         73.60176211453744,
         107.4819383259912
        ]
       }
      },
      "rules": {
       "a": {
        "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c"
       },
       "blockquote": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-size": 3
       },
       "code": {
        "font-family": "Anonymous Pro"
       },
       "h1": {
        "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "font-family": "Merriweather",
        "font-size": 8
       },
       "h2": {
        "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "font-family": "Merriweather",
        "font-size": 6
       },
       "h3": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-family": "Lato",
        "font-size": 5.5
       },
       "h4": {
        "color": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "font-family": "Lato",
        "font-size": 5
       },
       "h5": {
        "font-family": "Lato"
       },
       "h6": {
        "font-family": "Lato"
       },
       "h7": {
        "font-family": "Lato"
       },
       "li": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-size": 3.25
       },
       "pre": {
        "font-family": "Anonymous Pro",
        "font-size": 4
       }
      },
      "text-base": {
       "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
       "font-family": "Lato",
       "font-size": 4
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
